import numpy as np
import random

# Define the environment
num_servers = 3  # Number of cloud servers
num_actions = num_servers  # Actions correspond to selecting a server
num_states = 100  # CPU utilization states (0-100%)

# Initialize Q-table
q_table = np.zeros((num_states, num_actions))

# Hyperparameters
alpha = 0.1  # Learning rate
gamma = 0.9  # Discount factor
epsilon = 1.0  # Exploration rate
epsilon_decay = 0.99
min_epsilon = 0.1

# Simulated server CPU utilizations
server_cpu = [random.randint(10, 30) for _ in range(num_servers)]  # Initial utilizations

# Reward function
def get_reward(server_index, workload):
    if server_cpu[server_index] + workload > 100:
        return -10  # Penalty for overload
    return 10 - server_cpu[server_index]  # Higher reward for less utilized servers

# Update server utilizations
def update_server_utilization(server_index, workload):
    server_cpu[server_index] += workload
    server_cpu[server_index] = max(0, server_cpu[server_index] - random.randint(5, 15))  # Simulate usage decay

# Main training loop
num_episodes = 1000
workload_range = (5, 20)  # Workload size range

for episode in range(num_episodes):
    state = min(int(np.mean(server_cpu)), num_states - 1)  # Average CPU utilization as state

    for _ in range(10):  # Simulate multiple workload arrivals per episode
        # Select action (server) using epsilon-greedy policy
        if random.uniform(0, 1) < epsilon:
            action = random.randint(0, num_actions - 1)  # Explore
        else:
            action = np.argmax(q_table[state, :])  # Exploit

        # Simulate workload
        workload = random.randint(*workload_range)

        # Calculate reward
        reward = get_reward(action, workload)

        # Update Q-value
        next_state = min(int(np.mean(server_cpu)), num_states - 1)
        q_table[state, action] = q_table[state, action] + alpha * (
            reward + gamma * np.max(q_table[next_state, :]) - q_table[state, action]
        )

        # Update server utilization and state
        update_server_utilization(action, workload)
        state = next_state

    # Decay epsilon
    epsilon = max(min_epsilon, epsilon * epsilon_decay)

    if (episode + 1) % 100 == 0:
        print(f"Episode {episode + 1}/{num_episodes} - Epsilon: {epsilon:.2f} - Server Utilizations: {server_cpu}")

# Testing the trained model
print("\nTesting the trained model:\n")
test_workloads = [random.randint(*workload_range) for _ in range(20)]

for workload in test_workloads:
    state = min(int(np.mean(server_cpu)), num_states - 1)
    action = np.argmax(q_table[state, :])  # Select the best server based on the trained Q-table
    print(f"Workload: {workload} directed to Server {action + 1} (Utilization before: {server_cpu[action]}%)")
    update_server_utilization(action, workload)
    print(f"Server {action + 1} Utilization after: {server_cpu[action]}%\n")
